{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: API details.\n",
    "output-file: losses.html\n",
    "title: Losses\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp losses\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os, math, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MMD_loss(nn.Module):\n",
    "    '''\n",
    "    https://github.com/ZongxianLee/MMD_Loss.Pytorch/blob/master/mmd_loss.py\n",
    "    '''\n",
    "    def __init__(self, kernel_mul = 2.0, kernel_num = 5):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        return\n",
    "    \n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2) \n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = self.guassian_kernel(source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX + YY - XY -YX)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import ot\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "class OT_loss(nn.Module):\n",
    "    _valid = 'emd sinkhorn sinkhorn_knopp_unbalanced'.split()\n",
    "\n",
    "    def __init__(self, which='emd', use_cuda=True, detach_mass=True, detach_dist_for_plan=True, sinkhorn_lambda=2.0, covariance_lambda=0.0):\n",
    "        if which not in self._valid:\n",
    "            raise ValueError(f'{which} not known ({self._valid})')\n",
    "        elif which == 'emd':\n",
    "            self.fn = lambda m, n, M: ot.emd(m, n, M)\n",
    "        elif which == 'sinkhorn':\n",
    "            self.fn = lambda m, n, M : ot.sinkhorn(m, n, M, sinkhorn_lambda)\n",
    "        elif which == 'sinkhorn_knopp_unbalanced':\n",
    "            self.fn = lambda m, n, M : ot.unbalanced.sinkhorn_knopp_unbalanced(m, n, M, 1.0, 1.0)\n",
    "        else:\n",
    "            pass\n",
    "        self.use_cuda=use_cuda\n",
    "        self.detach_dist_for_plan = detach_dist_for_plan\n",
    "        self.detach_mass = detach_mass\n",
    "        self.covariance_lambda = covariance_lambda\n",
    "\n",
    "    def __call__(self, source, target, source_mass=None, target_mass=None, use_cuda=None, return_plan=False):\n",
    "        \"\"\"\n",
    "        DEPRECATING the use_cuda argument. Now inferring from the source and target.\n",
    "        \"\"\"\n",
    "        # if use_cuda is None:\n",
    "            # use_cuda = self.use_cuda\n",
    "        if source_mass is None:\n",
    "            mu = torch.tensor(ot.unif(source.size()[0]), dtype=source.dtype, device=source.device)\n",
    "        else:\n",
    "            mu = (source_mass)/(source_mass).sum()\n",
    "        if target_mass is None:\n",
    "            nu = torch.tensor(ot.unif(target.size()[0]), dtype=target.dtype, device=target.device)\n",
    "        else:\n",
    "            nu = (target_mass)/(target_mass).sum()\n",
    "        M = torch.cdist(source, target)**2\n",
    "        if self.detach_dist_for_plan:\n",
    "            # pi = self.fn(mu, nu, M.detach().cpu())\n",
    "            pi = self.fn(mu, nu, M.detach())\n",
    "        else:\n",
    "            pi = self.fn(mu, nu, M)\n",
    "        if type(pi) is np.ndarray:\n",
    "            pi = torch.tensor(pi)\n",
    "        elif type(pi) is torch.Tensor:\n",
    "            if self.detach_mass:\n",
    "                pi = pi.clone().detach()\n",
    "            # pi = pi.cuda() if use_cuda else pi\n",
    "        # M = M.to(pi.device)\n",
    "        loss = torch.sum(pi * M)\n",
    "        \n",
    "        if self.covariance_lambda > 0:\n",
    "            loss += self.covariance_lambda * covariance_loss(source, target)\n",
    "        \n",
    "        if return_plan:\n",
    "            return loss, pi\n",
    "        else:\n",
    "            return loss\n",
    "        \n",
    "        \n",
    "def ot_loss_given_plan(plan, source, target):\n",
    "    M = torch.cdist(source, target)**2\n",
    "    loss = torch.sum(plan * M)\n",
    "    return loss\n",
    "\n",
    "def covariance_loss(source, target):\n",
    "    # Center the data\n",
    "    source_centered = source - source.mean(dim=0, keepdim=True)\n",
    "    target_centered = target - target.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Compute empirical covariance matrices (using unbiased estimate)\n",
    "    cov_source = source_centered.t() @ source_centered / (source.size(0) - 1)\n",
    "    cov_target = target_centered.t() @ target_centered / (target.size(0) - 1)\n",
    "    \n",
    "    # Compute Frobenius norm of the difference\n",
    "    loss = torch.norm(cov_source - cov_target, p='fro')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_covariance_loss():\n",
    "    # Test with simple tensors\n",
    "    source = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float32)\n",
    "    target = torch.tensor([[2.0, 3.0], [4.0, 5.0], [6.0, 7.0]], dtype=torch.float32)\n",
    "    \n",
    "    loss = covariance_loss(source, target)\n",
    "    \n",
    "    # Basic checks\n",
    "    assert isinstance(loss, torch.Tensor)\n",
    "    assert loss.ndim == 0  # scalar\n",
    "    assert loss >= 0  # Frobenius norm is non-negative\n",
    "    \n",
    "    # Test with identical inputs\n",
    "    loss_identical = covariance_loss(source, source)\n",
    "    assert loss_identical.item() < 1e-6  # Should be very close to 0\n",
    "    \n",
    "    # Test with different sized inputs\n",
    "    source_big = torch.randn(10, 2)\n",
    "    target_big = torch.randn(5, 2)\n",
    "    loss_diff_size = covariance_loss(source_big, target_big)\n",
    "    assert isinstance(loss_diff_size, torch.Tensor)\n",
    "    \n",
    "    # Test invariance to constant shifts\n",
    "    shifted_source = source + 1.0\n",
    "    shifted_target = target + 1.0\n",
    "    loss_shifted = covariance_loss(shifted_source, shifted_target)\n",
    "    assert torch.abs(loss - loss_shifted) < 1e-6  # Should be equal\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_covariance_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xingzhi/.local/share/mamba/envs/env/lib/python3.11/site-packages/ot/bregman/_sinkhorn.py:667: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Unit tests\n",
    "def test_ot_loss():\n",
    "    # Test initialization\n",
    "    ot_loss = OT_loss(which='emd', use_cuda=False)\n",
    "    assert ot_loss.use_cuda == False\n",
    "\n",
    "    # Test with simple tensors\n",
    "    source = torch.tensor([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]], dtype=torch.float32)\n",
    "    target = torch.tensor([[0.0, 0.0], [2.0, 2.0], [4.0, 4.0]], dtype=torch.float32)\n",
    "    # Test basic loss computation\n",
    "    loss = ot_loss(source, target, use_cuda=False)\n",
    "    assert isinstance(loss, torch.Tensor)\n",
    "    assert loss.ndim == 0  # scalar\n",
    "    assert loss >= 0  # OT loss should be non-negative\n",
    "\n",
    "    # Test with custom masses\n",
    "    source_mass = torch.tensor([0.7, 0.3, 0.2], dtype=torch.float32)\n",
    "    target_mass = torch.tensor([0.4, 0.6, 0.0], dtype=torch.float32)\n",
    "    loss_with_mass = ot_loss(source, target, source_mass, target_mass, use_cuda=False)\n",
    "    assert isinstance(loss_with_mass, torch.Tensor)\n",
    "\n",
    "    # Test return_plan option\n",
    "    loss_with_plan, plan = ot_loss(source, target, use_cuda=False, return_plan=True)\n",
    "    assert isinstance(plan, torch.Tensor)\n",
    "    assert plan.shape == (3, 3)  # For 2x2 input tensors\n",
    "\n",
    "    # Test ot_loss_given_plan\n",
    "    plan = torch.tensor([[0.5, 0.5, 0.0], [0.5, 0.5, 0.0], [0.0, 0.0, 1.0]], dtype=torch.float32)\n",
    "    loss_manual = ot_loss_given_plan(plan, source, target)\n",
    "    assert isinstance(loss_manual, torch.Tensor)\n",
    "    assert loss_manual >= 0\n",
    "\n",
    "    # Test different OT variants\n",
    "    for method in ['sinkhorn', 'sinkhorn_knopp_unbalanced']:\n",
    "        ot_loss = OT_loss(which=method, use_cuda=False)\n",
    "        loss = ot_loss(source, target, use_cuda=False)\n",
    "        assert isinstance(loss, torch.Tensor)\n",
    "        assert loss >= 0\n",
    "\n",
    "    # Test invalid initialization\n",
    "    try:\n",
    "        OT_loss(which='invalid_method')\n",
    "        assert False, \"Should have raised ValueError\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "test_ot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "class Density_loss(nn.Module):\n",
    "    def __init__(self, hinge_value=0.01):\n",
    "        self.hinge_value = hinge_value\n",
    "        pass\n",
    "\n",
    "    def __call__(self, source, target, groups = None, to_ignore = None, top_k = 5):\n",
    "        if groups is not None:\n",
    "            # for global loss\n",
    "            c_dist = torch.stack([\n",
    "                torch.cdist(source[i], target[i]) \n",
    "                # NOTE: check if this should be 1 indexed\n",
    "                for i in range(1,len(groups))\n",
    "                if groups[i] != to_ignore\n",
    "            ])\n",
    "        else:\n",
    "            # for local loss\n",
    "             c_dist = torch.stack([\n",
    "                torch.cdist(source, target)                 \n",
    "            ])\n",
    "        values, _ = torch.topk(c_dist, top_k, dim=2, largest=False, sorted=False)\n",
    "        values -= self.hinge_value\n",
    "        values[values<0] = 0\n",
    "        loss = torch.mean(values)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Local_density_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sources, targets, groups, to_ignore, top_k = 5):\n",
    "        # print(source, target)\n",
    "        # c_dist = torch.cdist(source, target) \n",
    "        c_dist = torch.stack([\n",
    "            torch.cdist(sources[i], targets[i]) \n",
    "            # NOTE: check if should be from range 1 or not.\n",
    "            for i in range(1, len(groups))\n",
    "            if groups[i] != to_ignore\n",
    "        ])\n",
    "        vals, inds = torch.topk(c_dist, top_k, dim=2, largest=False, sorted=False)\n",
    "        values = vals[inds[inds]]\n",
    "        loss = torch.mean(values)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1789918865.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def energy_loss(model):\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def energy_loss(func, x, t): # should use the current t. i.e. t_seq[-1]\n",
    "    dxdt = func(t, x)\n",
    "    return torch.square(dxdt).mean()\n",
    "\n",
    "def energy_loss_seq(func, xtseq, t):\n",
    "    dxdt = torch.stack([func(t[i], xtseq[i]) for i in range(len(t))])\n",
    "    res = torch.square(dxdt).mean()\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
