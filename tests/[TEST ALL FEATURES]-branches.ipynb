{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab964908",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f8bb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MIOFlow.utils import generate_steps, set_seeds, config_criterion\n",
    "from MIOFlow.models import make_model, GrowthRateModel\n",
    "from MIOFlow.plots import plot_comparision, plot_losses\n",
    "from MIOFlow.train import train_ae, training_regimen\n",
    "from MIOFlow.constants import ROOT_DIR, DATA_DIR, NTBK_DIR, IMGS_DIR, RES_DIR\n",
    "from MIOFlow.datasets import (\n",
    "    make_diamonds, make_swiss_roll, make_tree, make_eb_data, \n",
    "    make_dyngen_data\n",
    ")\n",
    "from MIOFlow.geo import setup_distance\n",
    "from MIOFlow.exp import setup_exp\n",
    "from MIOFlow.eval import generate_plot_data\n",
    "\n",
    "import os, pandas as pd, numpy as np, \\\n",
    "    seaborn as sns, matplotlib as mpl, matplotlib.pyplot as plt, \\\n",
    "    torch, torch.nn as nn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d191189",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c734643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def branch_data_clean(t, e):\n",
    "    shapes = (len(e), len(t))\n",
    "    t = t.reshape(1,-1)\n",
    "    e = e.reshape(-1, 1)\n",
    "    ts = np.tile(t, (shapes[0], 1))[...,None]\n",
    "    es = np.tile(e, (1, shapes[1]))[...,None]\n",
    "    x = np.tile(t, (shapes[0], 1))\n",
    "    y = e * t**2\n",
    "    data = np.stack([x,y], axis=2)\n",
    "    data = np.concatenate((ts, es, data), axis=-1)\n",
    "    data = data.reshape(shapes[0]*shapes[1], -1)\n",
    "    return data\n",
    "def branch_data_data(data, n_colors=5, repeats=5, noisex=0.05, noisey=0.05, seed=32):\n",
    "    data = np.tile(data, (repeats,1))\n",
    "    df = pd.DataFrame(data, columns=['t', 'e1', 'd1', 'd2'])\n",
    "    np.random.seed(seed)\n",
    "    df['d1'] += np.random.randn(df.shape[0]) * noisex\n",
    "    df['d2'] += np.random.randn(df.shape[0]) * noisey\n",
    "    _, bin_edges = np.histogram(df['t'], bins=n_colors)\n",
    "    bin_indices = np.digitize(df['t'], bin_edges)\n",
    "    bin_indices[bin_indices > n_colors] = n_colors\n",
    "    df.insert(0, 'samples', bin_indices)\n",
    "    df.drop(columns=['t'], inplace=True)\n",
    "    df.samples -=1\n",
    "    return df\n",
    "def make_branch_cond(nt=20, ne=3, n_colors=5, repeats=10, noisex=0.1, noisey=0.1, seed=32):\n",
    "    t = np.linspace(0,1,nt)\n",
    "    e = np.linspace(-1,1,ne)\n",
    "    data = branch_data_clean(t, e)\n",
    "    df = branch_data_data(data, n_colors, repeats, noisex, noisey, seed)\n",
    "    return df\n",
    "\n",
    "df = make_branch_cond(ne=2, repeats=20)\n",
    "\n",
    "df = df[['samples', 'd1', 'd2', 'e1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2365263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e731bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 8), dpi=300)\n",
    "sns.scatterplot(data=df, x='d1', y='d2', hue='samples', palette='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 8), dpi=300)\n",
    "sns.scatterplot(data=df, x='d1', y='d2', hue='e1', palette='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c636d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "# df = df.drop(columns=['e1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3a634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df[['d1', 'd2']].mean(axis=0)\n",
    "stds = df[['d1', 'd2']].std(axis=0)\n",
    "df[['d1', 'd2']] = (df[['d1', 'd2']] - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8dc35b",
   "metadata": {},
   "source": [
    "# Train autoencoder or the geodesic embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bfa25",
   "metadata": {},
   "source": [
    "#### Set seeds and check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5a86628",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(0)\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341dd47",
   "metadata": {},
   "source": [
    "#### Handle hold-out training condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74c56d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is True if we want to holdout (or skip) one timepoint during training. It is used to test the accuracy of the trajectories on unseen data.\n",
    "hold_one_out = False\n",
    "# It can be a group number or 'random', works in tandem with hold_one_out\n",
    "hold_out = 3\n",
    "\n",
    "# The dimensions in the input space, it is columns - 1 because we assume one column is equal to \"samples\".\n",
    "model_features = len(df.columns) - 1 - 1 # drop the condition\n",
    "groups = sorted(df.samples.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72828508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These determine the logic flow for training: \n",
    "# #   use_emb=True use_gae=False is only the encoder to match the approximation of the geodesic.\n",
    "# #   use_emb=False use_gae=True the full Geodesic Autoencoder (GAE), i.e. matching the geodesic and a reconstruction loss.\n",
    "# #   use_emb=False use_gae=False Is not using the GAE.\n",
    "# #   use_emb=True use_gae=True, is redundant and should raise an error. \n",
    "# use_emb = False\n",
    "# use_gae = False\n",
    "\n",
    "# need_to_train_gae = (use_emb or use_gae) and use_emb != use_gae\n",
    "\n",
    "# # If the reconstruction loss needs to be computed.\n",
    "# recon = use_gae and not use_emb \n",
    "\n",
    "# # These are training GAE hyperparameters needed for training\n",
    "# # Distance_type in ['gaussian', 'alpha_decay'], and Gaussian scale\n",
    "# distance_type = 'gaussian'\n",
    "# rbf_length_scale=0.1\n",
    "# dist = setup_distance(distance_type, rbf_length_scale=rbf_length_scale)\n",
    "\n",
    "# #Can be changed depending on the dataset\n",
    "# n_epochs_emb = 1000\n",
    "# samples_size_emb = (30, )\n",
    "\n",
    "# # Layers for the Geodesic Autoencoder\n",
    "# gae_embedded_dim = 3\n",
    "# encoder_layers = [model_features, 8, gae_embedded_dim]\n",
    "\n",
    "# gae = Autoencoder(\n",
    "#     encoder_layers = encoder_layers,\n",
    "#     decoder_layers = encoder_layers[::-1],\n",
    "#     activation='ReLU', use_cuda = use_cuda\n",
    "# )\n",
    "# optimizer = torch.optim.AdamW(gae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef704313",
   "metadata": {},
   "source": [
    "# Specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81445197",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(10)\n",
    "\n",
    "#Directory where results are saved\n",
    "exp_name = 'test'\n",
    "\n",
    "# density loss knn\n",
    "use_density_loss = True\n",
    "\n",
    "# Weight of density (not percentage of total loss)\n",
    "# lambda_density = 35\n",
    "lambda_density = 0\n",
    "\n",
    "# For petal=LeakyReLU / dyngen=CELU\n",
    "activation = 'LeakyReLU'\n",
    "\n",
    "# Can change but we never really do, mostly depends on the dataset.\n",
    "layers = [16,32,16]\n",
    "\n",
    "# Scale of the noise in the trajectories. Either len(groups)*[float] or None. Should be None if using an adaptative ODE solver.\n",
    "sde_scales = len(groups)*[0.1] \n",
    "\n",
    "\n",
    "# model = make_model(\n",
    "#     model_features, layers, which='sde', method='euler',\n",
    "#     activation=activation, scales=sde_scales, use_cuda=use_cuda\n",
    "# )\n",
    "\n",
    "# model = make_model(\n",
    "#     model_features, layers, which='sde', method='reversible_heun', sde_type='stratonovich', adjoint_method='adjoint_reversible_heun',\n",
    "#     activation=activation, scales=sde_scales, use_cuda=use_cuda, use_norm=False\n",
    "# )\n",
    "\n",
    "\n",
    "model = make_model(\n",
    "    model_features, layers, which='sde', method='euler', sde_type='ito', \n",
    "    activation=activation, scales=sde_scales, use_cuda=use_cuda, use_norm=True, n_conditions=1, momentum_beta=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fefda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83e7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically \"batch size\"\n",
    "# sample_size=(60, )\n",
    "sample_size=(60, )\n",
    "\n",
    "# Training specification\n",
    "n_local_epochs = 40\n",
    "n_epochs = 0\n",
    "n_post_local_epochs = 0\n",
    "\n",
    "# Using the reverse trajectories to train\n",
    "reverse_schema = True\n",
    "# each reverse_n epoch\n",
    "reverse_n = 2\n",
    "\n",
    "\n",
    "criterion_name = 'ot'\n",
    "criterion = config_criterion(criterion_name, covariance_lambda=0.0, detach_dist_for_plan=False, detach_mass=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# Bookkeeping variables\n",
    "batch_losses = []\n",
    "globe_losses = []\n",
    "if hold_one_out and hold_out in groups:\n",
    "    local_losses = {f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups) if hold_out not in [t0, t1]}\n",
    "else:\n",
    "    local_losses = {f'{t0}:{t1}':[] for (t0, t1) in generate_steps(groups)}\n",
    "\n",
    "# For creating output.\n",
    "n_points = 100\n",
    "n_trajectories = 100\n",
    "n_bins = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38de32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = './results/test_all/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4fdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198eda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_model = GrowthRateModel(\n",
    "    feature_dims=2,\n",
    "    condition_dims=1,\n",
    "    layers=[64,64],\n",
    "    activation='LeakyReLU',\n",
    "    use_time=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "local_losses, batch_losses, globe_losses = training_regimen(\n",
    "    # local, global, local train structure\n",
    "    n_local_epochs=n_local_epochs, \n",
    "    n_epochs=n_epochs, \n",
    "    n_post_local_epochs=n_post_local_epochs,\n",
    "    \n",
    "    # where results are stored\n",
    "    exp_dir=exp_dir, \n",
    "\n",
    "    # BEGIN: train params\n",
    "    model=model, df=df, groups=groups, optimizer=optimizer, \n",
    "    criterion=criterion, use_cuda=use_cuda,\n",
    "    \n",
    "    hold_one_out=hold_one_out, hold_out=hold_out,\n",
    "    \n",
    "    use_density_loss=use_density_loss, \n",
    "    lambda_density=0.2,\n",
    "    # lambda_density=10,\n",
    "    \n",
    "    autoencoder=None, use_emb=False, use_gae=False, \n",
    "    \n",
    "    sample_size=sample_size, logger=None,\n",
    "    reverse_schema=False, reverse_n=reverse_n, # sde cant use reverse schema for now\n",
    "    # use_penalty=True, \n",
    "    use_penalty=True, \n",
    "    # lambda_energy=0.001,\n",
    "    lambda_energy_local = {g:1e-3 for g in groups},\n",
    "    lambda_energy_global = {g:1e-3 for g in groups},\n",
    "    # END: train params\n",
    "\n",
    "    plot_every=5,\n",
    "    n_points=n_points, n_trajectories=n_trajectories, n_bins=n_bins, \n",
    "    #local_losses=local_losses, batch_losses=batch_losses, globe_losses=globe_losses\n",
    "    lambda_cond=1.,\n",
    ")\n",
    "# run_time = time.time() - start_time + run_time_geo if use_emb or use_gae else time.time() - start_time\n",
    "# logger.info(f'Total run time: {np.round(run_time, 5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(\n",
    "    local_losses, batch_losses, globe_losses, \n",
    "    save=True, path=exp_dir, file='losses.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ae0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ad3c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nflo (Python 3.11.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e3594",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'nflo (Python 3.11.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MIOFlow.utils import sample\n",
    "\n",
    "\n",
    "data_t0 = sample(df, np.min(groups), replace=False, to_torch=True, use_cuda=use_cuda)\n",
    "\n",
    "sample_time = np.linspace(np.min(groups), np.max(groups), n_bins)\n",
    "sample_time = torch.tensor(sample_time, dtype=data_t0.dtype, device=data_t0.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_traj = model(data_t0, sample_time, return_whole_sequence=True)\n",
    "    # x_trajs.append(x_traj)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "scatter = ax.scatter(df['d1'], df['d2'],\n",
    "                c=df['samples'], cmap='viridis', s=10, alpha=.5)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "for i,traj in enumerate(np.transpose(x_traj, axes=(1,0,2))):\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=.5, linewidth=.5, color='Black')\n",
    "    ax.annotate('', xy=(traj[-1, 0], traj[-1, 1]), xytext=(traj[-2, 0], traj[-2, 1]),\n",
    "                    arrowprops=dict(arrowstyle=\"-|>\", color='Black', lw=1))\n",
    "ax.set_title('Trajectory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "groups = sorted(df['samples'].unique())\n",
    "sample_time = np.linspace(np.min(groups), np.max(groups), n_bins)\n",
    "from MIOFlow.utils import sample\n",
    "np.random.seed(32)\n",
    "data_t0 = sample(\n",
    "    df, np.min(groups), size=(n_points, ), \n",
    "    replace=False, to_torch=True, use_cuda=use_cuda\n",
    ")\n",
    "# data_t0 = torch.tensor([[-1., 0.]], dtype=torch.float32, device='cpu')\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "sample_time = torch.tensor(sample_time, dtype=data_t0.dtype, device=data_t0.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_traj = model(data_t0, sample_time, return_whole_sequence=True)\n",
    "\n",
    "scatter = ax.scatter(df['d1'], df['d2'],\n",
    "                c=df['samples'], cmap='viridis', s=10, alpha=.5)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "for i,traj in enumerate(np.transpose(x_traj, axes=(1,0,2))):\n",
    "    ax.plot(traj[:, 0], traj[:, 1], alpha=.5, linewidth=.5, color='Black')\n",
    "    ax.annotate('', xy=(traj[-1, 0], traj[-1, 1]), xytext=(traj[-2, 0], traj[-2, 1]),\n",
    "                 arrowprops=dict(arrowstyle=\"-|>\", color='Black', lw=1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b676fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f8711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
